# Working with JSON & SQL in Python

---

# Part 1 — Working with JSON in Pandas

---

## 1️⃣ Intuition

JSON (JavaScript Object Notation) is a **structured text format** used to store and transfer data.

It looks like Python dictionaries + lists.

Example:

```json
{
  "id": 1,
  "name": "Akshit",
  "skills": ["Python", "ML"],
  "details": {
    "age": 22,
    "city": "Ahmedabad"
  }
}
```

Important:

* JSON supports **nested structures**
* Real APIs return JSON
* Most production ML pipelines consume JSON

If you don’t understand nested JSON, you’ll struggle with APIs.

---

## 2️⃣ Opening a JSON File

### Basic

```python
import pandas as pd

df = pd.read_json("data.json")
```

---

## 3️⃣ Important Parameters of `pd.read_json()`

Now don’t just memorize. Understand why they exist.

---

### 1. `orient`

Defines how JSON is structured.

Common types:

| orient    | When Used                              |
| --------- | -------------------------------------- |
| `records` | List of dictionaries                   |
| `columns` | Default table format                   |
| `index`   | Index-based structure                  |
| `split`   | Separate keys for index, columns, data |
| `values`  | Only values                            |

Example:

```python
pd.read_json("data.json", orient="records")
```

If you choose wrong orient → wrong DataFrame shape.

---

### 2. `lines=True`

Used when file is in JSON Lines format (each row = one JSON object)

Very common in big data.

Example file:

```
{"id":1,"name":"A"}
{"id":2,"name":"B"}
```

Use:

```python
pd.read_json("data.json", lines=True)
```

If you forget this → you’ll get parsing error.

---

### 3. `dtype`

Force column types.

Why important?

Because JSON doesn’t enforce strong typing.

```python
pd.read_json("data.json", dtype={"age": int})
```

Production impact:
Wrong dtype → model training errors.

---

### 4. `convert_dates`

Automatically parse date columns.

```python
pd.read_json("data.json", convert_dates=["created_at"])
```

---

### 5. `encoding`

For special characters.

```python
pd.read_json("data.json", encoding="utf-8")
```

---

### 6. `chunksize`

For large JSON files.

```python
for chunk in pd.read_json("data.json", lines=True, chunksize=1000):
    process(chunk)
```

Industry thinking:
Never load 10GB file fully into memory.

---

## 4️⃣ Handling Nested JSON

This is where beginners fail.

Use:

```python
pd.json_normalize(data)
```

Example:

```python
import json

with open("data.json") as f:
    data = json.load(f)

df = pd.json_normalize(data)
```

If nested inside another key:

```python
pd.json_normalize(data, record_path="orders")
```

Real-world example:
E-commerce orders API response.

---

## 5️⃣ When JSON Fails

* Deeply nested objects
* Inconsistent schema
* Missing keys
* Mixed data types

Production rule:
Validate schema before using.

---

# Part 2 — Working with SQL (MySQL) in Python

This is more important than JSON for job roles.

Real companies store data in databases, not CSV files.

---

# What is MySQL?

MySQL is a relational database management system.

* Stores structured data
* Uses SQL (Structured Query Language)
* Used in almost every production system

---

# Connecting to MySQL using mysql-connector

Install:

```bash
pip install mysql-connector-python
```

---

## 1️⃣ Basic Connection

```python
import mysql.connector

conn = mysql.connector.connect(
    host="localhost",
    user="root",
    password="password",
    database="company_db"
)
```

---

## 2️⃣ Important Connection Parameters

| Parameter          | Meaning             |
| ------------------ | ------------------- |
| host               | Server address      |
| user               | Username            |
| password           | Password            |
| database           | DB name             |
| port               | Default 3306        |
| auth_plugin        | Authentication type |
| connection_timeout | Timeout seconds     |

Example:

```python
mysql.connector.connect(
    host="localhost",
    user="root",
    password="pass",
    database="company",
    port=3306,
    connection_timeout=10
)
```

Industry thinking:
Always set timeout in production.

---

## 3️⃣ Executing Query

```python
cursor = conn.cursor()

cursor.execute("SELECT * FROM employees")

results = cursor.fetchall()
```

---

## 4️⃣ Using Pandas with SQL

Cleaner way:

```python
import pandas as pd

df = pd.read_sql("SELECT * FROM employees", conn)
```

Better for ML pipelines.

---

# real_sql_query Parameters (Important)

Used in:

```python
pd.read_sql_query(sql, con)
```

### 1. `sql`

Can be:

* Raw query string
* SQLAlchemy object

Example:

```python
query = "SELECT id, salary FROM employees WHERE salary > 50000"
```

---

### 2. `con`

Connection object.

---

### 3. `params`

For parameterized queries (VERY IMPORTANT for security).

Wrong way (SQL injection risk):

```python
query = f"SELECT * FROM users WHERE id = {user_id}"
```

Correct way:

```python
query = "SELECT * FROM users WHERE id = %s"
df = pd.read_sql_query(query, conn, params=(user_id,))
```

Production rule:
Never build SQL using string formatting.

---

### 4. `chunksize`

For large queries:

```python
for chunk in pd.read_sql_query(query, conn, chunksize=1000):
    process(chunk)
```

---

### 5. `parse_dates`

Parse date columns:

```python
pd.read_sql_query(query, conn, parse_dates=["created_at"])
```

---

# Real Production Considerations

You’re aiming for ML Engineer roles. So think deeper.

---

## 1️⃣ Connection Pooling

Opening connection every time is slow.

Use connection pooling in real systems.

---

## 2️⃣ Indexing Matters

If database table is not indexed:

Query will be slow.

ML engineer must:
Understand data size + query cost.

---

## 3️⃣ Data Leakage Risk

If you pull entire database blindly:

You may leak PII data.

Always select only needed columns.

---

## 4️⃣ Scaling Issues

If query returns 10 million rows:

* Use chunksize
* Or aggregate in SQL first

Better:

```sql
SELECT department, AVG(salary)
FROM employees
GROUP BY department;
```

Instead of loading full data into Python.

Push computation to database.

---

# JSON vs SQL (Industry Comparison)

| Feature     | JSON                  | SQL                   |
| ----------- | --------------------- | --------------------- |
| Structure   | Semi-structured       | Structured            |
| Used in     | APIs                  | Databases             |
| Nested      | Yes                   | No                    |
| Best for    | Config, API responses | Large relational data |
| Performance | Slower for analytics  | Optimized for queries |

---

# When to Use What?

Use JSON when:

* Working with APIs
* Log files
* Config files

Use SQL when:

* Training ML models on company data
* Aggregations
* Joins across tables
* Production pipelines

---

# Brutal Truth

If you:

* Only know CSV
* Don’t know joins
* Don’t know parameterized queries
* Don’t understand schema

You are not ML engineer ready.

You are notebook-level only.

---